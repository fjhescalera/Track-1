{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google's Data Analytics Capstone Project - Track 1\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this project, we'll act as a junior data analyst working on the marketing analyst team at Bellabeat, a high-tech manufacturer of health-focused\n",
    "products for women. Where Urška Sršen, cofounder and Chief Creative Officer of Bellabeat, believes that analyzing smart\n",
    "device fitness data could help unlock new growth opportunities for the company. We have been asked to analyze smart device data to gain insights into how customers are using their devices and help guide the marketing strategy for Bellabeat.\n",
    "\n",
    "For this project, we'll refer from using Excel or sheets since the objective is to showcase SQL, R and Tableau (also because we don't have an Excel license on the PC we're working with). We'll limit to opening files with Excel to peek at the info and the structure.\n",
    "\n",
    "## Ask\n",
    "\n",
    "We'll begin by defining our business task:\n",
    "\n",
    "`Identify trends in smart device usage and use these trends to influence Bellabeat marketing strategy`\n",
    "\n",
    "Which we can then break down into the following questions:\n",
    "* What are some trends in smart device usage?\n",
    "* How could these trends apply to Bellabeat customers?\n",
    "* How could these trends help influence Bellabeat marketing strategy?\n",
    "\n",
    "## Prepare\n",
    "\n",
    "We are referred to the following dataset:\n",
    " <a href=\"https://www.kaggle.com/datasets/arashnic/fitbit\" target=\"_blank\">Fitbit fitness tracker</a>(Public domain, available through Kaggle) \n",
    "Which contains data from fitbit users, including information for physical activity, heart rate, weight and sleep monitoring. This information is divided in 18 CSV documents which contain the information at different levels such as daily, hourly or by minute and also it is arranged in wide and long format.\n",
    "\n",
    "We begin importing these documents in BigQuery and after some exploratory analysis we encounter certain limitations with this dataset:\n",
    "* Neither gender nor age are disclosed for users\n",
    "* With only 33 users and data for 1 month the sample size is too small to make valid assumptions\n",
    "* Not all segments (sleep, activity, weight) have the same number of users contributing their data\n",
    "* Some datasets do not include information for the units they're measuring with (distance, instensity,fat)\n",
    "\n",
    "![Total users in daily activity](img/users_daily_activity.png)\n",
    "\n",
    "Total users in activity data\n",
    "\n",
    "![Total users in sleep activity](img/users_sleep_activity.png)\n",
    "\n",
    "Total users in sleep data\n",
    "\n",
    "To address these limitations we can search for more complete datasets or work under some assumptions. While looking for other datasets we come across the <a href=\"https://zenodo.org/record/53894#.X9oeh3Uzaao\" target=\"_blank\">original source</a> for this data which contains an additional month of information. We also note that the researchers used a company called Fitabase to source this data, upon looking at their website we come across a <a href=\"https://www.fitabase.com/media/1930/fitabasedatadictionary102320.pdf\" target=\"_blank\">data dictionary</a> that will be helpful in clarifying some questions we had about the data such as what units correspond to some columns in different datasets.\n",
    "\n",
    "### Considerations\n",
    "\n",
    "As the second dataset only contains data in long format and anyway it's easier to work with this format we'll ignore the wide format files, also in the original 18 CSV files there are 3 files which contain daily data for steps, intensities and calories which we'll also ignore since this information is summarized in the daily activity file. Another file we won't be using for this analysis is the one for sleep records by minute, which contains information about the quality of sleep at night for the users, instead we'll use the SleepDay file that contains the amount of time a person is sleeping by day, though we have to note again that this file is not present in the 2nd dataset so we'll only analize 1 month for this data.\n",
    "\n",
    "\n",
    "We will be appending the 2 datasets where applicable and also will group the data by time section (we'll explain this in the next phase of the project) therefore the final data that we will be working with is the following:\n",
    "\n",
    "|Data file|Summary|Time Period|\n",
    "|---|---|---|\n",
    "|Daily Activity  |Daily steps, distance(km), calories and intensity by distance/minutes|2016-03-12 - 2016-05-12|\n",
    "|Heart Rate Activity|Heart rate values by the minute|2016-03-12 - 2016-05-12|\n",
    "|Hourly Activity|Hourly steps, calories and total/average intensities |2016-03-12 - 2016-05-12|\n",
    "|METs|Metabolic equivalents by the minute|2016-03-12 - 2016-05-12|\n",
    "|Weight Logs|Weight (kg/lbs), fat, BMI and boolean for whether it was a manual report|2016-03-30 - 2016-05-12|\n",
    "|Sleep Activity|Daily total times asleep, minutes asleep and minutes in bed|2016-04-12 - 2016-05-12|\n",
    "\n",
    "## Process\n",
    "\n",
    "### Parsing dates\n",
    "\n",
    "When we first open the CSV files contained in the dataset we identify the date variables as a timestamp, however when trying to upload them to BigQuery most of them can't be parsed as they are. This is because, according to the source, there are differences in format between devices and personal preferences too. To work around this, we can import them as strings, parse and separate them in another SQL query with a temp table and the help of the functions: `PARSE_DATETIME(format_string, datetime_string)` and `EXTRACT(part FROM datetime_expression)`.\n",
    "\n",
    "![Parsing and extracting query for heartrate data](img/heartrate_parsed_query.png)\n",
    "\n",
    "Even when having different formats for datetime in the same table these functions can parse them correctly. We also have to note that the discovery of the 2nd part of the dataset was made after importing most of the files from the 1st part. This isn't a problem though, since we can easily append the two datasets in each query for parsing the dates with `UNION ALL`. Also, the use of a temp table can be skipped and just generate the query with the parsing of dates, some testing is needed to define which one is faster but the latter produces a more readable query.\n",
    "\n",
    "![Parsing and appending query for heartrate data](img/heartrate_parsed_union_query.png)\n",
    "\n",
    "### Connecting BigQuery with R\n",
    "Because some of the tables that we are dealing with have millions of rows and to ease on the computational load on our local machine we can connect to BigQuery from R and perform any query using BQ's computational power.\n",
    "\n",
    "Let's begin by loading our packages, in this project we'll use `DBI` and `bigrquery` to connect R to BigQuery. Also, well load `tidyverse` because we'll be using `dbplyr` to _translate_ our code into a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(DBI)\n",
    "library(bigrquery)\n",
    "library(tidyverse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our packages loaded we have to create a connection to our database, we do this with the help of `dbConnect()` which has the following syntax\n",
    "```R\n",
    "con <- dbConnect( \n",
    "bigrquery::bigquery(), \n",
    "project = \"PROJECT_ID\",\n",
    "dataset = DATASET_NAME, \n",
    "billing = PROJECT_ID )\n",
    "```\n",
    "Note that this will creat a connection to BigQuery and handle the auth from our web browser, there are ways to handle the auth without user input but that's outside our scope. Our connection is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "con <- dbConnect(\n",
    "bigrquery::bigquery(),\n",
    "project = \"warm-utility-374917\",\n",
    "dataset = \"Bellabeat\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other useful functions like `dbListTables()` to see what tables are present in our dataset and `bigrquery::bq_auth()` which can be used to handle our auth for example to change users.\n",
    "\n",
    "### Creating queries\n",
    "The way bigrquery handles queries is lazy, what this means is that the info is not stored locally and will not be stored until we use `collect()`. Also these queries are stored as a table element, we can see this in the class of any table we create with `tbl()`. This is useful because we can use dplyr verbs in the same way we would with a tibble or a dataframe and the package will translate it automatically to a query.\n",
    "\n",
    "\n",
    "Let's see this in action with one our tables, the heartrate activity table. We will begin by making a pointer to the table with `tbl()`, however we encounter the following error:\n",
    ">In as.integer.integer64(x) : NAs produced by integer overflow\n",
    "Which indicates that sinceour IDs are stored as integers the whole number cannot be displayed, we'll change this by using `mutate()` and `as.numeric()` to transform it to a numeric format. We continue building the query introducing a `distinct()` clause to drop possible duplicates, finally we include another `mutate()` to create a datetime column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in tbl(con, \"heartrate_activity_merged\") %>% mutate(Id = as.numeric(Id)) %>% : no se pudo encontrar la función \"%>%\"\n",
     "output_type": "error",
     "traceback": [
      "Error in tbl(con, \"heartrate_activity_merged\") %>% mutate(Id = as.numeric(Id)) %>% : no se pudo encontrar la función \"%>%\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "heartrate_activity <- tbl(con, \"heartrate_activity_merged\") %>%\n",
    "    mutate(Id=as.numeric(Id)) %>%\n",
    "    distinct() %>%\n",
    "    mutate(datetime=sql(\"DATETIME(date,time)\"))    \n",
    "class(heartrate_activity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the query generated by our previous code by including the function `show_query()`. Which gives us the following query:\n",
    "``` SQL\n",
    "SELECT *, DATETIME(date,time) AS `datetime`\n",
    "FROM (\n",
    "  SELECT DISTINCT SAFE_CAST(`Id` AS FLOAT64) AS `Id`, `date`, `time`, `Value`\n",
    "  FROM `heartrate_activity_merged`\n",
    ") `q01`\n",
    "```\n",
    "We now have out heartrate data in a table we can query with all the info correctly formatted. However there are other issues we need to address. First, this data is measured in an interval of 30-60 seconds which gives us a table of 3.4+ million rows. This is the reason why we are working with BigQuery from R, saving this locally would probably not be very fast depending on the specs of our local machine. Second, even if BQ has no trouble working with this volume of data there is a bill every time a query is performed. We have 1TB of processing quota every month and even if this query is ~113 MB everytime we query the table we can optimize this by grouping our data by the hour.\n",
    "\n",
    "We can address this creating a new table and querying from that table onwards. We'll use `as_data_frame()` to transform our table to a dataframe that can be then uploaded to BQ with `dbWriteTable()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "heartrate_activity %>%\n",
    "    mutate(hour=sql(\"DATETIME_TRUNC(datetime, hour)\")) %>%\n",
    "    select(Id,hour,Value) %>%\n",
    "    group_by(Id,hour ) %>%\n",
    "    summarise(avg_heartrate=mean(Value),min_heartrate=min(Value),max_heartrate=max(Value),num_readings=count(Value)) #%>%\n",
    "    as_data_frame() %>%\n",
    "    dbWriteTable(con,\"heartrate_activity_hourly\",.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the \"heartrate_activity_hourly\" table now in our BQ dataset we reduce the processed data from ~113 MB to ~10 MB and the number of rows from 3.4 million to 8499.\n",
    "\n",
    "We can then create another table that references our processed information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "heartrate_hourly_activity <- tbl(con,\"heartrate_activity_hourly\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
